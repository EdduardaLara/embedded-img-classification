{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdduardaLara/embedded-img-classification/blob/main/cnn_plant_diseases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0FdA6yFUZDTA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3c7EA6xCYFH"
      },
      "source": [
        "# Install some packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjC5q9ZbybQq",
        "outputId": "b9ce3a2c-72bf-46ae-8307-fb1b4463fb55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.16.0)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.8.0\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWMkIl4YCcbC"
      },
      "source": [
        "# Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpm5_5nGCbwP",
        "outputId": "4890efca-7cc1-4f3c-8c59-943e728b179a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq9Ah4xXJ6Yx"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fgk9vKOyJont"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
        "\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0zcUSOddmawQ"
      },
      "outputs": [],
      "source": [
        "# shutil.rmtree('/content/data/plantvillage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nWeFXI92miwT"
      },
      "outputs": [],
      "source": [
        "# shutil.rmtree('/content/results/plant-village')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfp5M2ST-Xq9"
      },
      "source": [
        "Explore some runtime ressources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grRX8STP9TNs",
        "outputId": "0a3d1fbe-299a-402e-f0c5-37916c8d497e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "List of GPUs Available:  []\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(\"List of GPUs Available: \", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mdbDD9oJ-Qc"
      },
      "source": [
        "# Global variables setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hOOJ-EoyT_jt"
      },
      "outputs": [],
      "source": [
        "PLANT_CULTURE = \"tomato\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oIyz4g0_RDbF"
      },
      "outputs": [],
      "source": [
        "DOWNLOAD_DATA = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CrHdaXQuTIUu"
      },
      "outputs": [],
      "source": [
        "DELETE_DATASET_FOLDER = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QBCvmSMdXWLx"
      },
      "outputs": [],
      "source": [
        "DELETE_SOME_SUBFOLDERS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0fB9y31KenH"
      },
      "source": [
        "Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vENGuBPpKw0k"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'plant-village'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UivbRsl7JnqQ",
        "outputId": "fcd5c41e-89fe-446d-9d0e-f201d272bff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project dir: /content | Data dir: /content/data/plant-village\n"
          ]
        }
      ],
      "source": [
        "project_dir = os.getcwd()\n",
        "data_dir: str = project_dir + '/' + 'data/' + dataset_name\n",
        "print(f\"Project dir: {project_dir} | Data dir: {data_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvc0mBRQKsSb",
        "outputId": "6164ca56-1835-4035-8b98-93d846ed1289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results dir: /content/drive/MyDrive/IC_VANT/PlantVillage/results\n"
          ]
        }
      ],
      "source": [
        "results_dir = '/content/drive/MyDrive/IC_VANT/PlantVillage/results'\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "print(f\"Results dir: {results_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoLOLiu4KgwQ"
      },
      "source": [
        "Model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dibn7XgGKiuL"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-D01gGiKmFp"
      },
      "source": [
        "Image parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BUGW4ZnhZCRz"
      },
      "outputs": [],
      "source": [
        "IMAGE_HEIGHT = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "n6rY88w2ZFe4"
      },
      "outputs": [],
      "source": [
        "IMAGE_WIDTH = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U0r3yqwbKpCM"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B62GqwmVJjpA"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uqwMyLvBJc3X"
      },
      "outputs": [],
      "source": [
        "def check_data_dir(silent_console: bool = True):\n",
        "    \"\"\"\n",
        "    check if the data_dir exists\n",
        "\n",
        "    :param:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if os.path.exists(data_dir):\n",
        "        print(\"Data_dir found !\") if not silent_console else None\n",
        "        return True\n",
        "    else:\n",
        "        print(\"The data_dir not found !\") if not silent_console else None\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztHH5QFsOiWz"
      },
      "source": [
        "## Dataset manip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZIO2NT14K_fE"
      },
      "outputs": [],
      "source": [
        "def get_dataset_info(directory: str) -> int:\n",
        "    \"\"\"\n",
        "    get the number of images in the dataset\n",
        "\n",
        "    :param directory: str\n",
        "    :return: int\n",
        "    \"\"\"\n",
        "    dir_path = Path(directory)\n",
        "    image_count = len(list(dir_path.glob('*/*.jpg')))\n",
        "    image_count += len(list(dir_path.glob('*/*.JPG')))\n",
        "    return image_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RuZelqFRX5A6"
      },
      "outputs": [],
      "source": [
        "def check_nb_of_data_in_dataset(dataset: tf.data.Dataset):\n",
        "    \"\"\"\n",
        "    check the number of data in the dataset\n",
        "\n",
        "    :param dataset: tf.data.Dataset\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    nb_of_batches = dataset.cardinality().numpy()\n",
        "    nb_of_data = nb_of_batches * BATCH_SIZE\n",
        "    print(f\"Nb of data: {nb_of_data} | Nb of batches: {nb_of_batches}\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W8_TO1brYSHf"
      },
      "outputs": [],
      "source": [
        "def check_nb_of_classes_in_dataset(dataset: tf.data.Dataset):\n",
        "    \"\"\"\n",
        "    check the number of classes in the dataset\n",
        "\n",
        "    :param dataset: tf.data.Dataset\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    class_names = dataset.class_names\n",
        "    print(f\"Nb of classes: {len(class_names)} | Class names: {class_names}\")\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "h4U1_GlALQrc"
      },
      "outputs": [],
      "source": [
        "def load_split_dataset(val_split: float, test_split: float, silent_console: bool = True):\n",
        "    \"\"\"\n",
        "    load and split the dataset\n",
        "\n",
        "    :return: tf.data.Dataset, tf.data.Dataset, tf.data.Dataset\n",
        "    \"\"\"\n",
        "    # get training dataset\n",
        "    eval_split = val_split + test_split\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        validation_split=eval_split,\n",
        "        subset=\"training\",\n",
        "        seed=123,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # get data to eval (validation and test)\n",
        "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        validation_split=eval_split,\n",
        "        subset=\"validation\",\n",
        "        seed=123,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    return train_ds, val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qFl57VfsMOBV"
      },
      "outputs": [],
      "source": [
        "def get_dataset_classes(dataset: tf.data.Dataset, dataset_type: str, silent_console: bool = True):\n",
        "    \"\"\"\n",
        "    check the dataset classes\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    class_names = dataset.class_names\n",
        "    if not silent_console:\n",
        "        print(f\"Dataset: {dataset_type} | Nb of classes: {len(class_names)} | Class names: {class_names}\")\n",
        "    return class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IXPMRRMxM_eY"
      },
      "outputs": [],
      "source": [
        "def check_batch_size(dataset: tf.data.Dataset, dataset_type: str):\n",
        "    \"\"\"\n",
        "    check the batch size of the dataset\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print(f\"\\n------ Checking batch size of the {dataset_type} dataset...\")\n",
        "    for image_batch, labels_batch in dataset:\n",
        "        print(f\"Image batch shape: {image_batch.shape}\")\n",
        "        print(f\"Label batch shape: {labels_batch.shape}\\n\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TFGMF9n8NDxZ"
      },
      "outputs": [],
      "source": [
        "def display_img_sample_of_dataset(dataset: tf.data.Dataset, dataset_type: str):\n",
        "    \"\"\"\n",
        "    display a sample of images from the dataset\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    class_names = dataset.class_names\n",
        "\n",
        "    # Take one batch of images and create a subplot\n",
        "    for images, labels in dataset.take(1):\n",
        "        for i in range(9):\n",
        "            ax = plt.subplot(3, 3, i + 1)  # Create the subplot\n",
        "            ax.imshow(images[i].numpy().astype(\"uint8\"))  # Show the image\n",
        "\n",
        "            # Set the title on the subplot (not the entire plot)\n",
        "            ax.set_title(f\"{class_names[labels[i]]}\", fontsize=8)\n",
        "            ax.axis(\"off\")  # Remove the axis labels\n",
        "\n",
        "    # Adjust layout to prevent overlapping titles\n",
        "    plt.subplots_adjust(top=0.9, bottom=0.1, left=0.1, right=0.9, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # Add a title to the entire plot\n",
        "    plt.suptitle(f\"Sample of images from the {dataset_type} dataset\", fontsize=16)\n",
        "\n",
        "    # Save the plot\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "    plt.savefig(results_dir + f\"sample_images_{dataset_type}.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ubHJYkoENWOv"
      },
      "outputs": [],
      "source": [
        "def set_prefetch(dataset: tf.data.Dataset):\n",
        "    \"\"\"\n",
        "    set the prefetch for the dataset\n",
        "\n",
        "    :param dataset: tf.data.Dataset\n",
        "    :return: tf.data.Dataset\n",
        "    \"\"\"\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "    return dataset.shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "H5v1sehCNais"
      },
      "outputs": [],
      "source": [
        "def normalize_dataset(dataset: tf.data.Dataset, silent_console: bool = True):\n",
        "    \"\"\"\n",
        "    Normalize image data.\n",
        "    RGB values are in the [0, 255] range, so we need to scale them to the [0, 1] range.\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print(\"Normalizing dataset...\")\n",
        "    normalizer_layer = keras.layers.Rescaling(1. / 255)\n",
        "    normalized_ds = dataset.map(lambda x, y: (normalizer_layer(x), y))\n",
        "    image_batch, labels_batch = next(iter(normalized_ds))\n",
        "\n",
        "    # get the first image to check the pixel values\n",
        "    if not silent_console:\n",
        "        first_image = image_batch[0]\n",
        "        print(f\"First image pixel values -> Min: {np.min(first_image)} | Max: {np.max(first_image)} | \"\n",
        "              f\"Shape: {first_image.shape}\")\n",
        "    return normalized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VOf0N2o2NqSE"
      },
      "outputs": [],
      "source": [
        "def show_first_data_in_dataset(dataset: tf.data.Dataset, class_names: list):\n",
        "    \"\"\"\n",
        "    Show the first data in the dataset\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print(\"\\n---- Showing the first data in the dataset...\")\n",
        "    # get the first batch of data\n",
        "    for image, label in dataset.take(1):\n",
        "        # Show the first image and label\n",
        "        first_img = image[0]\n",
        "        first_label = label[0]\n",
        "        print(f\"First image shape: {first_img.shape} | First label: {first_label}\")\n",
        "        print(f\"First image pixel values -> Min: {np.min(first_img)} | Max: {np.max(first_img)}\")\n",
        "\n",
        "        # Display the first image\n",
        "        plt.figure()\n",
        "        plt.imshow(first_img)\n",
        "        plt.title(f\"First image example | Label: {first_label} | Class name: {class_names[first_label]}\")\n",
        "        plt.grid(False)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqgPPc_IN-k4"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mIXLo8oERnSo"
      },
      "outputs": [],
      "source": [
        "def data_augmentation():\n",
        "    \"\"\"\n",
        "    Create a data augmentation layer\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        keras.layers.RandomRotation(0.2),\n",
        "    ])\n",
        "    return data_augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VccThl__N6sM"
      },
      "outputs": [],
      "source": [
        "def create_model(class_names: list, img_height: int, img_width: int):\n",
        "    \"\"\"\n",
        "    Create a CNN model to classify image\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_classes = len(class_names)\n",
        "    image_shape = (img_height, img_width, 3)\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=image_shape),\n",
        "        #data_augmentation(),\n",
        "        keras.layers.Reshape((img_height, img_width, 3)),\n",
        "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(num_classes, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Lkyhfd1hOAhQ"
      },
      "outputs": [],
      "source": [
        "def compile_mode(model: tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Compile the model\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "91qFwsEMOHnr"
      },
      "outputs": [],
      "source": [
        "def show_model_fit(hist: tf.keras.callbacks.History, nb_epochs: int):\n",
        "    \"\"\"\n",
        "    Show the model fit\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    acc = hist.history['accuracy']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "\n",
        "    loss = hist.history['loss']\n",
        "    val_loss = hist.history['val_loss']\n",
        "\n",
        "    epochs_range = range(nb_epochs)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    # Save the plot\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "    plt.savefig(results_dir + \"model_fit.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jN7TR5lUOIjS"
      },
      "outputs": [],
      "source": [
        "def save_model(model: tf.keras.Model, model_name: str, file_format: str = \"keras\"):\n",
        "    \"\"\"\n",
        "    Save the model\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "    model_file = model_name + \".\" + file_format\n",
        "    model_path = results_dir + model_file\n",
        "    model.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "SobR-_Q-sAF7"
      },
      "outputs": [],
      "source": [
        "def create_checkpoint_weights_callback():\n",
        "    \"\"\"\n",
        "    Create a checkpoint callback to save trained weights per epoch done.\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    checkpoint_dir = results_dir + \"/training_checkpoints\"\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_prefix,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "    return checkpoint_callback, checkpoint_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "pBlGuIsusxBG"
      },
      "outputs": [],
      "source": [
        "def weights_files_key(filename):\n",
        "    \"\"\"\n",
        "    Get the epoch number of each trained weight saved.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename: str\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nb of the epoch\n",
        "    \"\"\"\n",
        "    base_name = filename.split('.')[0]\n",
        "    epoch_number = base_name.split('_')[1]\n",
        "    print(f\"Epoch number: {epoch_number} | Base name: {base_name}\")\n",
        "    return int(epoch_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mq-vi-VJZVB"
      },
      "source": [
        "# Main code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNbk6uaRQvvE"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2hEurlcoPw1I",
        "outputId": "ddc6285c-1628-4ea3-fddb-81033ad4708b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/emmarex/plantdisease\n",
            "License(s): unknown\n",
            "Downloading plantdisease.zip to /content/data\n",
            " 99% 651M/658M [00:08<00:00, 126MB/s]\n",
            "100% 658M/658M [00:08<00:00, 78.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "if DOWNLOAD_DATA:\n",
        "  import kagglehub\n",
        "\n",
        "  # download\n",
        "  !kaggle datasets download -d emmarex/plantdisease -p /content/data/ --unzip\n",
        "\n",
        "  #rename data folder\n",
        "  !mv /content/data/PlantVillage /content/data/plant-village\n",
        "else:\n",
        "  print(\"Data already downloaded !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ucDoOGGQS7cd"
      },
      "outputs": [],
      "source": [
        "if DELETE_DATASET_FOLDER:\n",
        "\n",
        "  # Delete the folder and all its contents\n",
        "  shutil.rmtree('/content/data/plantvillage')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5EbesS_VNvW"
      },
      "source": [
        "Check subfolders in data_dir, delete all subdirectories with a culture different of our set culture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KsYAcvP5UOzI",
        "outputId": "94c532fd-6bd8-4b1f-d36a-5fcd7c71cbac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking subdirectory: /content/data/plant-village/Tomato_Early_blight\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato_Early_blight\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Pepper__bell___Bacterial_spot\n",
            "Moving subdirectory: /content/data/plant-village/Pepper__bell___Bacterial_spot to /content/data/plant-village-others\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Pepper__bell___healthy\n",
            "Moving subdirectory: /content/data/plant-village/Pepper__bell___healthy to /content/data/plant-village-others\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Tomato_Leaf_Mold\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato_Leaf_Mold\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Tomato_Spider_mites_Two_spotted_spider_mite\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato_Spider_mites_Two_spotted_spider_mite\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Tomato_Bacterial_spot\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato_Bacterial_spot\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Tomato__Tomato_YellowLeaf__Curl_Virus\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato__Tomato_YellowLeaf__Curl_Virus\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Potato___healthy\n",
            "Moving subdirectory: /content/data/plant-village/Potato___healthy to /content/data/plant-village-others\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Potato___Late_blight\n",
            "Moving subdirectory: /content/data/plant-village/Potato___Late_blight to /content/data/plant-village-others\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Tomato_Late_blight\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato_Late_blight\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Tomato_healthy\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato_healthy\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Tomato__Target_Spot\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato__Target_Spot\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Tomato__Tomato_mosaic_virus\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato__Tomato_mosaic_virus\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Tomato_Septoria_leaf_spot\n",
            "Keeping subdirectory: /content/data/plant-village/Tomato_Septoria_leaf_spot\n",
            "\n",
            "\n",
            "Checking subdirectory: /content/data/plant-village/Potato___Early_blight\n",
            "Moving subdirectory: /content/data/plant-village/Potato___Early_blight to /content/data/plant-village-others\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if DELETE_SOME_SUBFOLDERS:\n",
        "\n",
        "  def delete_subfolders(data_dir, target_culture):\n",
        "    for subdir in os.listdir(data_dir):\n",
        "        subdir_path = os.path.join(data_dir, subdir)\n",
        "        print(f\"Checking subdirectory: {subdir_path}\")\n",
        "        if os.path.isdir(subdir_path) and PLANT_CULTURE in subdir.lower():\n",
        "            print(f\"Keeping subdirectory: {subdir_path}\")\n",
        "        else:\n",
        "            # move subdir to plant-village-others\n",
        "            path = data_dir + '-others'\n",
        "            if not os.path.exists(path):\n",
        "                os.makedirs(path)\n",
        "            print(f\"Moving subdirectory: {subdir_path} to {path}\")\n",
        "            shutil.move(subdir_path, os.path.join(path, 'plant-village-others'))\n",
        "        print(\"\\n\")\n",
        "\n",
        "  delete_subfolders(data_dir, PLANT_CULTURE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRdboGRFOviT"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6_Mb2E1JDR2",
        "outputId": "acb99df8-f618-42c9-be5c-ffb73ca6a45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# PLANT-VILLAGE dataset contains 16010 images\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not check_data_dir():\n",
        "    raise ValueError(\"Data dir not found !\")\n",
        "nb_img_data = get_dataset_info(data_dir)\n",
        "print(f\"# {dataset_name.upper()} dataset contains {nb_img_data} images\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0jh8ZKALIX6",
        "outputId": "92f23f2d-f45b-4bec-deb1-1d3116ca79af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16011 files belonging to 10 classes.\n",
            "Using 9607 files for training.\n",
            "Found 16011 files belonging to 10 classes.\n",
            "Using 6404 files for validation.\n"
          ]
        }
      ],
      "source": [
        "train_dataset, val_dataset = load_split_dataset(0.2, 0.2, silent_console=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBrpnHZEMF4d"
      },
      "source": [
        "### Check dataset classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NPqUMTuMFc6",
        "outputId": "9d99787c-61cd-4661-a4a2-650459fc12b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nb of class: 10 | Classes: ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato__Tomato_mosaic_virus', 'Tomato_healthy']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_classes = get_dataset_classes(train_dataset, \"train\")\n",
        "val_classes = get_dataset_classes(val_dataset, \"validation\")\n",
        "if train_classes != val_classes:\n",
        "    raise ValueError(\"The classes in the train and validation datasets are different\")\n",
        "else:\n",
        "    print(f\"Nb of class: {len(train_classes)} | Classes: {train_classes}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wborzIe6M3E1"
      },
      "source": [
        "### Display sample of images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adVtZhg6NGKR"
      },
      "source": [
        "Train img sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g_f9hUUM2n5",
        "outputId": "093cfdff-f870-4f41-a30b-107a5f78884c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------ Checking batch size of the train dataset...\n",
            "Image batch shape: (4, 256, 256, 3)\n",
            "Label batch shape: (4,)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "check_batch_size(train_dataset, \"train\")\n",
        "# display_img_sample_of_dataset(train_dataset, \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISlKhq22NI7Y"
      },
      "source": [
        "Validation img sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGwnfpNmNNjv",
        "outputId": "8e1697e3-4d7a-4673-a610-9081e711c109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------ Checking batch size of the validation dataset...\n",
            "Image batch shape: (4, 256, 256, 3)\n",
            "Label batch shape: (4,)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "check_batch_size(val_dataset, \"validation\")\n",
        "# display_img_sample_of_dataset(val_dataset, \"validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCHnGdHKNPSi"
      },
      "source": [
        "## Data pre-processing and cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oQN6GBOPC9t"
      },
      "source": [
        "### Prefetch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl3xI5JKNSJv",
        "outputId": "8abb703d-03b4-47da-befc-2fafd579ff38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Setting prefetch for the dataset...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n---- Setting prefetch for the dataset...\")\n",
        "train_dataset = set_prefetch(train_dataset)\n",
        "val_dataset = set_prefetch(val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utGSRnffNeYP"
      },
      "source": [
        "### Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G011XsPvNYDk",
        "outputId": "7812356a-4ace-4e4f-9437-a51bd0295d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Normalizing dataset...\n",
            "Normalizing dataset...\n",
            "Normalizing dataset...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n---- Normalizing dataset...\")\n",
        "train_dataset_normalized = normalize_dataset(train_dataset)\n",
        "val_dataset_normalized = normalize_dataset(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "VVtV6fE7Nrib"
      },
      "outputs": [],
      "source": [
        "# show_first_data_in_dataset(train_dataset_normalized, train_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZpGDF0ENuB4"
      },
      "source": [
        "## Create and fit a CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgOxcf0nUwGi"
      },
      "source": [
        "### Visualize some augmented images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnCi3lwqPMha"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv54jISHvGW9",
        "outputId": "4ceaf398-563b-4c3a-e6be-0be314ae352e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Building the model...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n---- Building the model...\")\n",
        "model = create_model(train_classes, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "model = compile_mode(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaLBwUSxvJQH",
        "outputId": "3d464bd7-014e-4501-e556-c99a0efa1b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 256, 256, 3)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 127, 127, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 62, 62, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 30, 30, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 12, 12, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 6, 6, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 2, 2, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 184202 (719.54 KB)\n",
            "Trainable params: 184202 (719.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj7S6qbRq2G2"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "oon2wKUzq1v-"
      },
      "outputs": [],
      "source": [
        "start_ep = -1\n",
        "fit_model = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNfLJSnWrwsZ"
      },
      "source": [
        "Create callbacks to save fit by epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "UpekkchgrwaE"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback, checkpoint_dir = create_checkpoint_weights_callback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_RYcvA5yhqA",
        "outputId": "7a08ccfa-1a3a-4b29-fd3b-e52981ad8690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint_dir: /content/drive/MyDrive/IC_VANT/PlantVillage/results/training_checkpoints\n"
          ]
        }
      ],
      "source": [
        "print(f\"Checkpoint_dir: {checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTzLuctoscT3"
      },
      "source": [
        "Check entire model fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDLSqIPmvfzH",
        "outputId": "5ace9a29-4ab3-413b-be62-32e54faf6d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- No entire model saved !\n"
          ]
        }
      ],
      "source": [
        "if start_ep == -1:\n",
        "  # check the entire model\n",
        "  if os.path.exists(results_dir + \"cnn_first_model.keras\"):\n",
        "    print(\"\\n---- An entire model is already saved !\")\n",
        "    print(\"\\n---- Loading the model...\")\n",
        "    model = keras.models.load_model(results_dir + f\"cnn_model_ep={EPOCHS}.keras\")\n",
        "    fit_model = False\n",
        "  else:\n",
        "    start_ep = 0\n",
        "    print(\"\\n---- No entire model saved !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt9c12xJxEyL"
      },
      "source": [
        "Check checkpoints of epochs fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzVKCkUzvZKF",
        "outputId": "b74eca78-6507-4374-c85a-3ecff0019a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Checking ckpt results\n",
            "\n",
            " No ckpt model files found !\n"
          ]
        }
      ],
      "source": [
        "if start_ep == 0:\n",
        "  print(f\"\\n---- Checking ckpt results\")\n",
        "\n",
        "  if os.path.exists(checkpoint_dir):\n",
        "    files = os.listdir(checkpoint_dir)\n",
        "    if len(files) > 0:\n",
        "      print(f\"\\n We found some ckpt model files !\")\n",
        "\n",
        "      highest_epoch_file = max(files, key=weights_files_key)\n",
        "      highest_epoch = weights_files_key(highest_epoch_file)\n",
        "\n",
        "      # get the last epoch fitting\n",
        "      if highest_epoch < EPOCHS:\n",
        "        start_ep = highest_epoch\n",
        "        path_to_load_model = os.path.join(checkpoint_dir, \"ckpt_{}.weights.h5\".format(start_ep))\n",
        "        print(\"\\n---- Loading weights of: {}\".format(path_to_load_model))\n",
        "        model.load_weights(path_to_load_model)\n",
        "        fit_model = True\n",
        "      else:\n",
        "        fit_model = False\n",
        "        # load checkpoint fitting\n",
        "        path_to_load_model = os.path.join(checkpoint_dir, \"ckpt_{}.weights.h5\".format(start_ep))\n",
        "        print(\"\\n---- Loading weights of: {}\".format(path_to_load_model))\n",
        "        model.load_weights(path_to_load_model)\n",
        "        # save complete model\n",
        "        save_model(model, f\"cnn_model_ep={EPOCHS}\")\n",
        "        print(\"\\n---- Complete model saved !\")\n",
        "\n",
        "    else:\n",
        "      print(f\"\\n No ckpt model files found !\")\n",
        "      fit_model = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4DkeVQYxiiS"
      },
      "source": [
        "Fit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlKSiLPpbEPd",
        "outputId": "ff622d66-cf94-4bc5-bb8c-4dc1ecb1c116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "print(start_ep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejQRHb1xxPq-",
        "outputId": "d8735bd1-aa90-4742-c647-3f7c7b866fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Fitting the model...\n",
            "Start epoch: 0\n",
            "End epoch: 10\n",
            "\n",
            "Epoch 1/10\n",
            " 248/2402 [==>...........................] - ETA: 15:59 - loss: 2.1979 - accuracy: 0.1925"
          ]
        }
      ],
      "source": [
        "if fit_model:\n",
        "  print(\"\\n---- Fitting the model...\")\n",
        "  print(f\"Start epoch: {start_ep}\")\n",
        "  print(f\"End epoch: {EPOCHS}\\n\")\n",
        "\n",
        "  # fit using GPU\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    hist = model.fit(train_dataset_normalized,\n",
        "                    validation_data=val_dataset_normalized,\n",
        "                    epochs=EPOCHS,\n",
        "                    initial_epoch=start_ep,\n",
        "                    callbacks=[checkpoint_callback]\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oQ3leWU7CAWa"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n",
        "    print(layer.name, layer.output.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XLIffh2abMKt"
      },
      "outputs": [],
      "source": [
        "if fit_model:\n",
        "  show_model_fit(hist, EPOCHS)\n",
        "  save_model(model, f\"cnn_model_ep={EPOCHS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d_HI4ITOQOk"
      },
      "source": [
        "### Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v8JrJ1BUOXXW"
      },
      "outputs": [],
      "source": [
        "print(\"\\n---- Evaluating the model...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vj7lNNIQOSwa"
      },
      "outputs": [],
      "source": [
        "print(\"### Val dataset evaluation:\")\n",
        "val_score = model.evaluate(val_dataset_normalized)\n",
        "print(f\"Val accuracy: {val_score[1]*100.00:.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gzZFZC92ObpB"
      },
      "outputs": [],
      "source": [
        "print(\"### Train dataset evaluation:\")\n",
        "train_score = model.evaluate(train_dataset_normalized)\n",
        "print(f\"Train accuracy: {train_score[1]*100.00:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik0xXfWazgJs"
      },
      "source": [
        "## Create a Quantization Aware Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pnFDmvCZ0u-2"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_optimization as tfmot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0BEy37nuv_R"
      },
      "source": [
        "### Quantize layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aYUn-TZE02vd"
      },
      "outputs": [],
      "source": [
        "base_model = keras.models.clone_model(model)\n",
        "base_model.set_weights(model.get_weights())\n",
        "base_model = compile_mode(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LelYOBdl09nb"
      },
      "source": [
        "Test model copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rRLrEti_1Bq6"
      },
      "outputs": [],
      "source": [
        "print(\"### Train dataset evaluation (COPY MODEL):\")\n",
        "train_score = base_model.evaluate(train_dataset_normalized)\n",
        "print(f\"Train accuracy: {train_score[1]*100.00:.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P4PAkq0a3pVj"
      },
      "outputs": [],
      "source": [
        "print(type(base_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1gU-SsoE4X0t"
      },
      "outputs": [],
      "source": [
        "print(tf.__version__)  # Check TensorFlow version\n",
        "print(keras.__version__)  # Check Keras version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-nJOy90nDP"
      },
      "source": [
        "Quantize only the Dense, MaxPool2D, Conv2D Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S0xEmZN6uljm"
      },
      "outputs": [],
      "source": [
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsNEwiQuunZC"
      },
      "source": [
        "Apply quantization-aware training to specific layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lTXEmYwwzzGc"
      },
      "outputs": [],
      "source": [
        "quant_aware_model = quantize_model(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM4hoKwPup50"
      },
      "source": [
        "### Compile and fit the quantization model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUIYOymXu6ad"
      },
      "source": [
        "Compile the quantization model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jlKTTfd-u5j1"
      },
      "outputs": [],
      "source": [
        "quant_aware_model.compile(optimizer='adam',\n",
        "                          loss='sparse_categorical_crossentropy',\n",
        "                          metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM0lk0OuvBC-"
      },
      "source": [
        "Fit the quantization model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FlaXfmBuu4HU"
      },
      "outputs": [],
      "source": [
        "quant_aware_model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8hmifwlvGo-"
      },
      "source": [
        "### Save q-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SVwpBP3AvJFv"
      },
      "outputs": [],
      "source": [
        "save_model(quant_aware_model, f\"cnn_quant_aware_model_ep={EPOCHS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6cmk5J_vUlY"
      },
      "source": [
        "### Evaluate q-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_7b9mote08eZ"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] Calculating Quant Aware model accuracy\")\n",
        "scores = quant_aware_model.evaluate(val_dataset)\n",
        "print(f\"Val Accuracy: {round(scores[1],4)*100}%\")\n",
        "scores_train = quant_aware_model.evaluate(train_dataset)\n",
        "print(f\"Train Accuracy: {round(scores_train[1],4)*100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZB1tzYs1MYO"
      },
      "source": [
        "## TF to TFlite\n",
        "\n",
        "Convert model to tensorflowlite model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9GxFnhZj1Sh5"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSgFOKY91tuh"
      },
      "source": [
        "Save TFlite model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_KkiLFic1v3P"
      },
      "outputs": [],
      "source": [
        "quantized_tflite_model.save(results_dir + \"q_cnn_model.tflite\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}